{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer classification using CNN and Transfer Learning\n",
    "\n",
    "- In the United States, breast cancer continues to be the leading cause of cancer death among women of all races. \n",
    "- Studies have shown that improvement to survival rate over the last decade can be attributed to early diagnosis and awareness of better treatment options.\n",
    "- Convolutional Neural Networks (ConvNets or CNNs) are a category of Neural Networks that have proven very effective in areas such as image recognition and classification.\n",
    "- Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/palakagrawal/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#libraries to be imported\n",
    "\n",
    "#visualiztion\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "#general libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "#import tensorflow deep learning framework\n",
    "import tensorflow as tf\n",
    "\n",
    "#shuffle dataset\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#split the dataset\n",
    "from sklearn.cross_validation import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/palakagrawal/Desktop/Courses/ADS/agrawal_palak_spring18_ads/project\n"
     ]
    }
   ],
   "source": [
    "#Current path is fetched\n",
    "cur_path = os.path.dirname(os.getcwd())\n",
    "print(cur_path)\n",
    "file_path = cur_path + '/data/calc_case_description_train_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the csv file\n",
    "skin_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>breast density</th>\n",
       "      <th>left or right breast</th>\n",
       "      <th>image view</th>\n",
       "      <th>abnormality id</th>\n",
       "      <th>abnormality type</th>\n",
       "      <th>calc type</th>\n",
       "      <th>calc distribution</th>\n",
       "      <th>assessment</th>\n",
       "      <th>pathology</th>\n",
       "      <th>subtlety</th>\n",
       "      <th>image file path</th>\n",
       "      <th>cropped image file path</th>\n",
       "      <th>ROI mask file path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_00005</td>\n",
       "      <td>3</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>calcification</td>\n",
       "      <td>AMORPHOUS</td>\n",
       "      <td>CLUSTERED</td>\n",
       "      <td>3</td>\n",
       "      <td>MALIGNANT</td>\n",
       "      <td>3</td>\n",
       "      <td>Calc-Training_P_00005_RIGHT_CC/1.3.6.1.4.1.959...</td>\n",
       "      <td>Calc-Training_P_00005_RIGHT_CC_1/1.3.6.1.4.1.9...</td>\n",
       "      <td>Calc-Training_P_00005_RIGHT_CC_1/1.3.6.1.4.1.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P_00005</td>\n",
       "      <td>3</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>1</td>\n",
       "      <td>calcification</td>\n",
       "      <td>AMORPHOUS</td>\n",
       "      <td>CLUSTERED</td>\n",
       "      <td>3</td>\n",
       "      <td>MALIGNANT</td>\n",
       "      <td>3</td>\n",
       "      <td>Calc-Training_P_00005_RIGHT_MLO/1.3.6.1.4.1.95...</td>\n",
       "      <td>Calc-Training_P_00005_RIGHT_MLO_1/1.3.6.1.4.1....</td>\n",
       "      <td>Calc-Training_P_00005_RIGHT_MLO_1/1.3.6.1.4.1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_00007</td>\n",
       "      <td>4</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>calcification</td>\n",
       "      <td>PLEOMORPHIC</td>\n",
       "      <td>LINEAR</td>\n",
       "      <td>4</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>4</td>\n",
       "      <td>Calc-Training_P_00007_LEFT_CC/1.3.6.1.4.1.9590...</td>\n",
       "      <td>Calc-Training_P_00007_LEFT_CC_1/1.3.6.1.4.1.95...</td>\n",
       "      <td>Calc-Training_P_00007_LEFT_CC_1/1.3.6.1.4.1.95...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P_00007</td>\n",
       "      <td>4</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>1</td>\n",
       "      <td>calcification</td>\n",
       "      <td>PLEOMORPHIC</td>\n",
       "      <td>LINEAR</td>\n",
       "      <td>4</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>4</td>\n",
       "      <td>Calc-Training_P_00007_LEFT_MLO/1.3.6.1.4.1.959...</td>\n",
       "      <td>Calc-Training_P_00007_LEFT_MLO_1/1.3.6.1.4.1.9...</td>\n",
       "      <td>Calc-Training_P_00007_LEFT_MLO_1/1.3.6.1.4.1.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P_00008</td>\n",
       "      <td>1</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>calcification</td>\n",
       "      <td>NaN</td>\n",
       "      <td>REGIONAL</td>\n",
       "      <td>2</td>\n",
       "      <td>BENIGN_WITHOUT_CALLBACK</td>\n",
       "      <td>3</td>\n",
       "      <td>Calc-Training_P_00008_LEFT_CC/1.3.6.1.4.1.9590...</td>\n",
       "      <td>Calc-Training_P_00008_LEFT_CC_1/1.3.6.1.4.1.95...</td>\n",
       "      <td>Calc-Training_P_00008_LEFT_CC_1/1.3.6.1.4.1.95...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patient_id  breast density left or right breast image view  abnormality id  \\\n",
       "0    P_00005               3                RIGHT         CC               1   \n",
       "1    P_00005               3                RIGHT        MLO               1   \n",
       "2    P_00007               4                 LEFT         CC               1   \n",
       "3    P_00007               4                 LEFT        MLO               1   \n",
       "4    P_00008               1                 LEFT         CC               1   \n",
       "\n",
       "  abnormality type    calc type calc distribution  assessment  \\\n",
       "0    calcification    AMORPHOUS         CLUSTERED           3   \n",
       "1    calcification    AMORPHOUS         CLUSTERED           3   \n",
       "2    calcification  PLEOMORPHIC            LINEAR           4   \n",
       "3    calcification  PLEOMORPHIC            LINEAR           4   \n",
       "4    calcification          NaN          REGIONAL           2   \n",
       "\n",
       "                 pathology  subtlety  \\\n",
       "0                MALIGNANT         3   \n",
       "1                MALIGNANT         3   \n",
       "2                   BENIGN         4   \n",
       "3                   BENIGN         4   \n",
       "4  BENIGN_WITHOUT_CALLBACK         3   \n",
       "\n",
       "                                     image file path  \\\n",
       "0  Calc-Training_P_00005_RIGHT_CC/1.3.6.1.4.1.959...   \n",
       "1  Calc-Training_P_00005_RIGHT_MLO/1.3.6.1.4.1.95...   \n",
       "2  Calc-Training_P_00007_LEFT_CC/1.3.6.1.4.1.9590...   \n",
       "3  Calc-Training_P_00007_LEFT_MLO/1.3.6.1.4.1.959...   \n",
       "4  Calc-Training_P_00008_LEFT_CC/1.3.6.1.4.1.9590...   \n",
       "\n",
       "                             cropped image file path  \\\n",
       "0  Calc-Training_P_00005_RIGHT_CC_1/1.3.6.1.4.1.9...   \n",
       "1  Calc-Training_P_00005_RIGHT_MLO_1/1.3.6.1.4.1....   \n",
       "2  Calc-Training_P_00007_LEFT_CC_1/1.3.6.1.4.1.95...   \n",
       "3  Calc-Training_P_00007_LEFT_MLO_1/1.3.6.1.4.1.9...   \n",
       "4  Calc-Training_P_00008_LEFT_CC_1/1.3.6.1.4.1.95...   \n",
       "\n",
       "                                  ROI mask file path  \n",
       "0  Calc-Training_P_00005_RIGHT_CC_1/1.3.6.1.4.1.9...  \n",
       "1  Calc-Training_P_00005_RIGHT_MLO_1/1.3.6.1.4.1....  \n",
       "2  Calc-Training_P_00007_LEFT_CC_1/1.3.6.1.4.1.95...  \n",
       "3  Calc-Training_P_00007_LEFT_MLO_1/1.3.6.1.4.1.9...  \n",
       "4  Calc-Training_P_00008_LEFT_CC_1/1.3.6.1.4.1.95...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the head\n",
    "skin_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Developed a one_hot function to classify the breast cancer image data in classes as BENIGN and MALIGNANT **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function for one hot encoding\n",
    "def one_hot(clas):\n",
    "    if clas == 'BENIGN':\n",
    "        c=0\n",
    "        return np.eye(2, dtype=float)[c] \n",
    "        \n",
    "    if clas == 'BENIGN_WITHOUT_CALLBACK':\n",
    "        c=0\n",
    "        return np.eye(2, dtype=float)[c]\n",
    "    \n",
    "    if clas == 'MALIGNANT':\n",
    "        c=1\n",
    "        return np.eye(2, dtype=float)[c]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Applied above function to classify data into classes, see column 'encode' **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skin_df['encode'] = skin_df['pathology'].apply(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>breast density</th>\n",
       "      <th>left or right breast</th>\n",
       "      <th>image view</th>\n",
       "      <th>abnormality id</th>\n",
       "      <th>abnormality type</th>\n",
       "      <th>calc type</th>\n",
       "      <th>calc distribution</th>\n",
       "      <th>assessment</th>\n",
       "      <th>pathology</th>\n",
       "      <th>subtlety</th>\n",
       "      <th>image file path</th>\n",
       "      <th>cropped image file path</th>\n",
       "      <th>ROI mask file path</th>\n",
       "      <th>encode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>P_02572</td>\n",
       "      <td>2</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>calcification</td>\n",
       "      <td>AMORPHOUS</td>\n",
       "      <td>CLUSTERED</td>\n",
       "      <td>0</td>\n",
       "      <td>MALIGNANT</td>\n",
       "      <td>2</td>\n",
       "      <td>Calc-Training_P_02572_LEFT_CC/1.3.6.1.4.1.9590...</td>\n",
       "      <td>Calc-Training_P_02572_LEFT_CC_1/1.3.6.1.4.1.95...</td>\n",
       "      <td>Calc-Training_P_02572_LEFT_CC_1/1.3.6.1.4.1.95...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>P_02572</td>\n",
       "      <td>2</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>1</td>\n",
       "      <td>calcification</td>\n",
       "      <td>AMORPHOUS</td>\n",
       "      <td>CLUSTERED</td>\n",
       "      <td>0</td>\n",
       "      <td>MALIGNANT</td>\n",
       "      <td>2</td>\n",
       "      <td>Calc-Training_P_02572_LEFT_MLO/1.3.6.1.4.1.959...</td>\n",
       "      <td>Calc-Training_P_02572_LEFT_MLO_1/1.3.6.1.4.1.9...</td>\n",
       "      <td>Calc-Training_P_02572_LEFT_MLO_1/1.3.6.1.4.1.9...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>P_02584</td>\n",
       "      <td>1</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>CC</td>\n",
       "      <td>1</td>\n",
       "      <td>calcification</td>\n",
       "      <td>PLEOMORPHIC</td>\n",
       "      <td>SEGMENTAL</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>4</td>\n",
       "      <td>Calc-Training_P_02584_LEFT_CC/1.3.6.1.4.1.9590...</td>\n",
       "      <td>Calc-Training_P_02584_LEFT_CC_1/1.3.6.1.4.1.95...</td>\n",
       "      <td>Calc-Training_P_02584_LEFT_CC_1/1.3.6.1.4.1.95...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>P_02584</td>\n",
       "      <td>1</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>1</td>\n",
       "      <td>calcification</td>\n",
       "      <td>PLEOMORPHIC</td>\n",
       "      <td>SEGMENTAL</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "      <td>4</td>\n",
       "      <td>Calc-Training_P_02584_LEFT_MLO/1.3.6.1.4.1.959...</td>\n",
       "      <td>Calc-Training_P_02584_LEFT_MLO_1/1.3.6.1.4.1.9...</td>\n",
       "      <td>Calc-Training_P_02584_LEFT_MLO_1/1.3.6.1.4.1.9...</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>P_01549</td>\n",
       "      <td>3</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>MLO</td>\n",
       "      <td>1</td>\n",
       "      <td>calcification</td>\n",
       "      <td>PLEOMORPHIC</td>\n",
       "      <td>CLUSTERED</td>\n",
       "      <td>4</td>\n",
       "      <td>MALIGNANT</td>\n",
       "      <td>2</td>\n",
       "      <td>Calc-Training_P_01549_LEFT_MLO/1.3.6.1.4.1.959...</td>\n",
       "      <td>Calc-Training_P_01549_LEFT_MLO_1/1.3.6.1.4.1.9...</td>\n",
       "      <td>Calc-Training_P_01549_LEFT_MLO_1/1.3.6.1.4.1.9...</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     patient_id  breast density left or right breast image view  \\\n",
       "1541    P_02572               2                 LEFT         CC   \n",
       "1542    P_02572               2                 LEFT        MLO   \n",
       "1543    P_02584               1                 LEFT         CC   \n",
       "1544    P_02584               1                 LEFT        MLO   \n",
       "1545    P_01549               3                 LEFT        MLO   \n",
       "\n",
       "      abnormality id abnormality type    calc type calc distribution  \\\n",
       "1541               1    calcification    AMORPHOUS         CLUSTERED   \n",
       "1542               1    calcification    AMORPHOUS         CLUSTERED   \n",
       "1543               1    calcification  PLEOMORPHIC         SEGMENTAL   \n",
       "1544               1    calcification  PLEOMORPHIC         SEGMENTAL   \n",
       "1545               1    calcification  PLEOMORPHIC         CLUSTERED   \n",
       "\n",
       "      assessment  pathology  subtlety  \\\n",
       "1541           0  MALIGNANT         2   \n",
       "1542           0  MALIGNANT         2   \n",
       "1543           0     BENIGN         4   \n",
       "1544           0     BENIGN         4   \n",
       "1545           4  MALIGNANT         2   \n",
       "\n",
       "                                        image file path  \\\n",
       "1541  Calc-Training_P_02572_LEFT_CC/1.3.6.1.4.1.9590...   \n",
       "1542  Calc-Training_P_02572_LEFT_MLO/1.3.6.1.4.1.959...   \n",
       "1543  Calc-Training_P_02584_LEFT_CC/1.3.6.1.4.1.9590...   \n",
       "1544  Calc-Training_P_02584_LEFT_MLO/1.3.6.1.4.1.959...   \n",
       "1545  Calc-Training_P_01549_LEFT_MLO/1.3.6.1.4.1.959...   \n",
       "\n",
       "                                cropped image file path  \\\n",
       "1541  Calc-Training_P_02572_LEFT_CC_1/1.3.6.1.4.1.95...   \n",
       "1542  Calc-Training_P_02572_LEFT_MLO_1/1.3.6.1.4.1.9...   \n",
       "1543  Calc-Training_P_02584_LEFT_CC_1/1.3.6.1.4.1.95...   \n",
       "1544  Calc-Training_P_02584_LEFT_MLO_1/1.3.6.1.4.1.9...   \n",
       "1545  Calc-Training_P_01549_LEFT_MLO_1/1.3.6.1.4.1.9...   \n",
       "\n",
       "                                     ROI mask file path      encode  \n",
       "1541  Calc-Training_P_02572_LEFT_CC_1/1.3.6.1.4.1.95...  [0.0, 1.0]  \n",
       "1542  Calc-Training_P_02572_LEFT_MLO_1/1.3.6.1.4.1.9...  [0.0, 1.0]  \n",
       "1543  Calc-Training_P_02584_LEFT_CC_1/1.3.6.1.4.1.95...  [1.0, 0.0]  \n",
       "1544  Calc-Training_P_02584_LEFT_MLO_1/1.3.6.1.4.1.9...  [1.0, 0.0]  \n",
       "1545  Calc-Training_P_01549_LEFT_MLO_1/1.3.6.1.4.1.9...  [0.0, 1.0]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skin_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n",
    "** Below method will convert images into matrices of pixels using Open CV library **\n",
    "- \n",
    "** Matrices for images and labels will be two things generated from load_train method **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to load the images from directory\n",
    "img_dir ='./dataset'\n",
    "\n",
    "def load_train(image_size):\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "\n",
    "    print('Reading training images')\n",
    "    \n",
    "    for i in range (len(skin_df.index)):\n",
    "        img = skin_df['image file path'][i]\n",
    "        path = os.path.join(img_dir,img)\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.resize(image, (image_size, image_size), cv2.INTER_LINEAR)\n",
    "        images.append(image)\n",
    "        labels.append(skin_df['encode'][i])\n",
    "        \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "\n",
    "    return images, labels, ids, cls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\n",
    "** Image size for CNN used is 128 and while exeuting Transfer learning same part can changed to 299 for image size **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training images\n"
     ]
    }
   ],
   "source": [
    "#loading the images\n",
    "images, labels = load_train(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "- Below four operations are main executed in ConvNet\n",
    "    - Convolution\n",
    "    - Non Linearity (ReLU)\n",
    "    - Pooling or Sub Sampling\n",
    "    - Classification (Fully Connected Layer)\n",
    "\n",
    "- ConvNets derive their name from the “convolution” operator. \n",
    "- The primary purpose of Convolution in case of a ConvNet is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data.\n",
    "\n",
    "\n",
    "- In CNN terminology, the 3×3 matrix is called a ‘filter‘ or ‘kernel’ or ‘feature detector’ and the matrix formed by sliding the filter over the image and computing the dot product is called the ‘Convolved Feature’ or ‘Activation Map’ or the ‘Feature Map‘. \n",
    "- It is important to note that filters acts as feature detectors from the original input image.\n",
    "- Different operations can be performed such as Edge Detection, Sharpen and Blur just by changing the numeric values of our filter matrix before the convolution operation\n",
    "- The more number of filters we have, the more image features get extracted and the better our network becomes at recognizing patterns in unseen images.\n",
    "\n",
    "\n",
    "- Below 4 layers are used out of which 4th is fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.\n",
    "filter_size1 = 3 \n",
    "num_filters1 = 16\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 3\n",
    "num_filters2 = 16\n",
    "\n",
    "# Convolutional Layer 3.\n",
    "filter_size3 = 3\n",
    "num_filters3 = 16\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 128             # Number of neurons in fully-connected layer.\n",
    "\n",
    "# Number of color channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 3\n",
    "\n",
    "# image dimensions (only squares for now)\n",
    "img_size = 128\n",
    "\n",
    "# Size of image when flattened to a single dimension\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "\n",
    "# batch size\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution and Pooling\n",
    "\n",
    "- Introducing Non-linearity\n",
    "    - ReLU stands for Rectified Linear Unit and is a non-linear operation.\n",
    "    - ReLU is an element wise operation (applied per pixel) and replaces all negative pixel values in the feature map by zero. \n",
    "    - The purpose of ReLU is to introduce non-linearity in our ConvNet, since most of the real-world data we would want our ConvNet to learn would be non-linear\n",
    "    - Other non linear functions such as tanh or sigmoid can also be used instead of ReLU, but ReLU has been found to perform better in most situations.\n",
    "    \n",
    "    \n",
    "- Pooling Step\n",
    "    - Spatial Pooling (also called subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information.\n",
    "    - In case of Max Pooling, we define a spatial neighborhood (for example, a 2×2 window) and take the largest element from the rectified feature map within that window. \n",
    "    - Instead of taking the largest element we could also take the average (Average Pooling) or sum of all elements in that window. \n",
    "    - In practice, Max Pooling has been shown to work better.\n",
    "    - Thus, we slide our 2 x 2 window by 2 cells (also called ‘stride’) and take the maximum value in each region.\n",
    "    \n",
    "![alt text](https://github.com/palakagrawal912/agrawal_palak_spring18_ads/tree/master/project/raw_images/pooling.png \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 49152])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x ,[-1, 128, 128, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv1\n",
    "W_conv1 = weight_variable([3, 3, 3, 16])\n",
    "b_conv1 = bias_variable([16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv2\n",
    "W_conv2 = weight_variable([3, 3, 16, 16])\n",
    "b_conv2 = bias_variable([16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv3\n",
    "W_conv3 = weight_variable([3, 3, 16, 16])\n",
    "b_conv3 = bias_variable([16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(16), Dimension(16), Dimension(16)])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_shape = h_pool3.get_shape()\n",
    "layer_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = layer_shape[1:4].num_elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flattening the layer\n",
    "layer_flat = tf.reshape(h_pool3, [-1, num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_7:0' shape=(?, 4096) dtype=float32>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1  = weight_variable([num_features, 128])\n",
    "b_fc1  = bias_variable([128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_fc1 = tf.nn.relu(tf.matmul(layer_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sha = h_fc1.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_fea = sha[1:4].num_elements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([num_fea, 3])\n",
    "b_fc2 = bias_variable([3])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross entropy\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training images\n"
     ]
    }
   ],
   "source": [
    "images, labels, ids, cls = load_train(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading images\n",
    "X = images\n",
    "Y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating train and test split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y,\n",
    "                                                  test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a batch\n",
    "def next_batch(num, data, labels):\n",
    "    \n",
    "    return data[0:num],labels[0:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.15625\n",
      "step 100, training accuracy 0.71875\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 1\n",
      "step 400, training accuracy 1\n",
      "step 500, training accuracy 1\n",
      "step 600, training accuracy 1\n",
      "step 700, training accuracy 1\n",
      "step 800, training accuracy 1\n",
      "step 900, training accuracy 1\n",
      "test accuracy 0.675325\n"
     ]
    }
   ],
   "source": [
    "# create a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        x_batch,y_batch = next_batch(32,X_train,Y_train)\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "              x: x_batch.reshape(-1,49152), y_: y_batch, keep_prob: 0.7})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: x_batch.reshape(-1,49152), y_: y_batch, keep_prob: 0.5})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "         x: X_val.reshape(-1,49152), y_: Y_val, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy reaches 100. This is due to overfitting. Overfitting could be reduced by changing the probability and image augmentation.The test accuracy os 67.5. Lets change the keep_prob and induce randomn in the batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomnizing the batch\n",
    "def next_batch(num, data, labels):\n",
    "    num_images = len(data)\n",
    "    idx = np.random.choice(num_images,size=num,replace=False)\n",
    "    \n",
    "    return data[idx],labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.25\n",
      "step 100, training accuracy 0.875\n",
      "step 200, training accuracy 0.75\n",
      "step 300, training accuracy 0.625\n",
      "step 400, training accuracy 0.65625\n",
      "step 500, training accuracy 0.59375\n",
      "step 600, training accuracy 0.71875\n",
      "step 700, training accuracy 0.5625\n",
      "step 800, training accuracy 0.65625\n",
      "step 900, training accuracy 0.75\n",
      "test accuracy 0.721707\n"
     ]
    }
   ],
   "source": [
    "# create a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        x_batch,y_batch = next_batch(32,X_train,Y_train)\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "              x: x_batch.reshape(-1,49152), y_: y_batch, keep_prob: 0.6})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: x_batch.reshape(-1,49152), y_: y_batch, keep_prob: 0.5})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "         x: X_val.reshape(-1,49152), y_: Y_val, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "We are using reclassification bottleneck to classify our images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Inception Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inception.data_dir = 'inception/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Inception v3 Model ...\n",
      "- Download progress: 100.0%\n",
      "Download finished. Extracting files.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "inception.maybe_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from inception import transfer_values_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = inception.Inception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath_train = \"./cache/inception_skin_train.pkl\"\n",
    "filepath_test  = \"./cache/inception_skin_test.pkl'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Inception transfer-values for training-images ...\n",
      "- Data loaded from cache-file: ./cache/inception_skin_train.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Inception transfer-values for training-images ...\")\n",
    "\n",
    "\n",
    "\n",
    "# If transfer-values have already been calculated then reload them,\n",
    "# otherwise calculate them and save them to a cache-file.\n",
    "transfer_values_train = transfer_values_cache(cache_path=filepath_train,\n",
    "                                              images=X_train,\n",
    "                                              model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Inception transfer-values for test-images ...\n",
      "- Data loaded from cache-file: ./cache/inception_skin_test.pkl'\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Inception transfer-values for test-images ...\")\n",
    "\n",
    "# If transfer-values have already been calculated then reload them,\n",
    "# otherwise calculate them and save them to a cache-file.\n",
    "transfer_values_test = transfer_values_cache(cache_path=filepath_test,\n",
    "                                             images=X_val,\n",
    "                                             model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1257, 2048)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_values_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(539, 2048)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer_values_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transfer_len = model.transfer_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, transfer_len], name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, shape=[None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1  = weight_variable([2048, 1024])\n",
    "b_fc1  = bias_variable([1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_fc1 = tf.nn.relu(tf.matmul(x, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc2  = weight_variable([1024, 128])\n",
    "b_fc2  = bias_variable([128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc3 = weight_variable([128, 1])\n",
    "b_fc3 = bias_variable([1])\n",
    "\n",
    "y_conv = tf.matmul(h_fc2_drop, W_fc3) + b_fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    num_images = len(data)\n",
    "    idx = np.random.choice(num_images,size=num,replace=False)\n",
    "    \n",
    "    return data[idx],labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.3125\n",
      "step 100, training accuracy 0.53125\n",
      "step 200, training accuracy 0.5\n",
      "step 300, training accuracy 0.65625\n",
      "step 400, training accuracy 0.5\n",
      "step 500, training accuracy 0.6875\n",
      "step 600, training accuracy 0.75\n",
      "step 700, training accuracy 0.84375\n",
      "step 800, training accuracy 0.65625\n",
      "step 900, training accuracy 0.6875\n",
      "step 1000, training accuracy 0.65625\n",
      "step 1100, training accuracy 0.75\n",
      "step 1200, training accuracy 0.8125\n",
      "step 1300, training accuracy 0.75\n",
      "step 1400, training accuracy 0.75\n",
      "step 1500, training accuracy 0.75\n",
      "step 1600, training accuracy 0.84375\n",
      "step 1700, training accuracy 0.625\n",
      "step 1800, training accuracy 0.71875\n",
      "step 1900, training accuracy 0.71875\n",
      "test accuracy 0.747681\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2000):\n",
    "        x_batch,y_batch = next_batch(32,transfer_values_train,Y_train)\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "              x: x_batch, y_: y_batch, keep_prob: 0.6})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: x_batch, y_: y_batch, keep_prob: 0.5})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "         x: transfer_values_test, y_: Y_val, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
